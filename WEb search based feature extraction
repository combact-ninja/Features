import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import re

# Function to scrape text data from web search results
def scrape_web_search_results(query, num_results=5):
    url = f"https://www.google.com/search?q={query}&num={num_results}"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    search_results = soup.find_all('div', class_='BNeawe vvjwJb AP7Wnd')
    search_text = ' '.join([result.get_text() for result in search_results])
    return search_text

# Function to preprocess text
def preprocess_text(text):
    # Remove special characters and numbers
    text = re.sub('[^a-zA-Z]', ' ', text)
    # Convert to lowercase
    text = text.lower()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = text.split()
    filtered_words = [word for word in words if word not in stop_words]
    text = ' '.join(filtered_words)
    return text

# Example query for web search
query = "Python"

# Scrape web search results
search_text = scrape_web_search_results(query)

# Preprocess the text
clean_text = preprocess_text(query)

# Feature extraction using CountVectorizer
vectorizer = CountVectorizer(max_features=1000)
X = vectorizer.fit_transform([clean_text]).toarray()
features = vectorizer.get_feature_names_out()

print("Features extracted:")
print(features)
