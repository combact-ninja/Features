import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import re

# Function to scrape text data from web search results
def scrape_web_search_results(query, num_results=5):
    url = f"https://www.google.com/search?q={query}&num={num_results}"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    search_results = soup.find_all('div', class_='BNeawe vvjwJb AP7Wnd')
    search_text = ' '.join([result.get_text() for result in search_results])
    return search_text

# Function to preprocess text
def preprocess_text(text):
    # Remove special characters and numbers
    text = re.sub('[^a-zA-Z]', ' ', text)
    # Convert to lowercase
    text = text.lower()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = text.split()
    filtered_words = [word for word in words if word not in stop_words]
    text = ' '.join(filtered_words)
    return text

# Example query for web search
query = "Python"

# Scrape web search results
search_text = scrape_web_search_results(query)

# Preprocess the text
clean_text = preprocess_text(query)

# Feature extraction using CountVectorizer
vectorizer = CountVectorizer(max_features=1000)
X = vectorizer.fit_transform([clean_text]).toarray()
features = vectorizer.get_feature_names_out()

print("Features extracted:")
print(features)


Explanation:

    The scrape_web_search_results function takes a query and number of results as input, constructs a Google search URL, and scrapes the text content from the search results using BeautifulSoup and requests libraries.

    The preprocess_text function preprocesses the scraped text by removing special characters, numbers, converting to lowercase, and removing stopwords using NLTK.

    The query variable contains the search query for which you want to extract features from the web search results.

    The search_text variable stores the scraped text from the web search results.

    The clean_text variable contains the preprocessed text ready for feature extraction.

    The CountVectorizer is used to extract features from the preprocessed text, limiting the number of features to 1000.

    The X variable stores the extracted features as a matrix, and features contains the feature names extracted by the CountVectorizer.






from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Sample data for demonstration
data = {
    'sentence': ['I love this product!', 'This product is terrible.', 'Great service!', 'Poor customer support.'],
    'sentiment': ['positive', 'negative', 'positive', 'negative']
}
df = pd.DataFrame(data)

# Feature extraction using CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['sentence']).toarray()

# Define labels
y = df['sentiment']

# Print the features (words)
print("Features (words):")
print(vectorizer.get_feature_names_out())

# Print the feature matrix
print("\nFeature matrix:")
print(X)

import re
from collections import Counter

def preprocess_text(text):
    # Remove special characters and numbers
    text = re.sub('[^a-zA-Z]', ' ', text)
    # Convert to lowercase
    text = text.lower()
    return text

def fbfe(sentence):
    sentence = preprocess_text(sentence)
    words = sentence.split()
    # Count the frequency of each word
    word_freq = Counter(words)
    return word_freq

# Sample sentence
sentence = "I love this product and I think it's amazing!"

# Perform Frequency Based Feature Extraction (FBFE)
features = fbfe(sentence)

print("Features (words) and their frequencies:")
print(features)



from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

# Sample data for demonstration
data = {
    'sentence': ['I love this product!', 'This product is terrible.', 'Great service!', 'Poor customer support.'],
    'sentiment': ['positive', 'negative', 'positive', 'negative']
}
df = pd.DataFrame(data)

# Define a list of sentiment words
sentiment_words = ['good', 'bad', 'great', 'terrible', 'love', 'poor', 'excellent', 'awesome']

# Function to preprocess text
def preprocess_text(text):
    # Remove special characters and numbers
    text = re.sub('[^a-zA-Z]', ' ', text)
    # Convert to lowercase
    text = text.lower()
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])
    return text

# Preprocess the sentences
df['clean_sentence'] = df['sentence'].apply(preprocess_text)

# Define custom tokenizer to include sentiment words
def custom_tokenizer(text):
    words = text.split()
    # Include sentiment words
    words.extend(sentiment_words)
    return words

# Feature extraction using CountVectorizer with custom tokenizer
vectorizer = CountVectorizer(tokenizer=custom_tokenizer, stop_words=stopwords.words('english'))
X = vectorizer.fit_transform(df['clean_sentence']).toarray()

# Define labels
y = df['sentiment']

# Print the features (words)
print("Features (words):")
print(vectorizer.get_feature_names_out())

# Print the feature matrix
print("\nFeature matrix:")
print(X)

